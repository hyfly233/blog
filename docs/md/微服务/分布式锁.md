# 分布式锁

## 分布式锁主流方案

- 基于数据库
- 基于 Redis
- 基于 Zookeeper
- etcd
- consul

1. 基于ZooKeeper的分布式锁，适用于高可靠（高可用）而并发量不是太大的场景；
2. 基于Redis的分布式锁，适用于并发量很大、性能要求很高的、而可靠性问题可以通过其他方案去弥补的场景。

## 分布式锁场景

- 商城秒杀
- 抢优惠券
- 接口幂等性校验

## Redis实现分布式锁的7种方案

- 方案一：SETNX + EXPIRE

```java
if（jedis.setnx(key_resource_id,lock_value) == 1）{ //加锁
    expire（key_resource_id，100; //设置过期时间
    try {
        //业务请求
    } catch() {
    } finally {
        jedis.del(key_resource_id); //释放锁
    }
}
```

setnx 和 expire 两个命令分开，不是原子操作，如果执行完 setnx 操作后进程挂掉，那么锁将会一直存在，其他线程永远获取不了锁



- 方案二：SETNX + value值是（系统时间+过期时间）

```java
long expires = System.currentTimeMillis() + expireTime; //系统时间 + 过期时间
String expiresStr = String.valueOf(expires);

// 如果当前锁不存在，返回加锁成功
if (jedis.setnx(key_resource_id, expiresStr) == 1) {
    return true;
} 
// 如果锁已经存在，获取锁的过期时间
String currentValueStr = jedis.get(key_resource_id);

// 如果获取到的过期时间，小于系统当前时间，表示已经过期
if (currentValueStr != null && Long.parseLong(currentValueStr) < System.currentTimeMillis()) {

    // 锁已过期，获取上一个锁的过期时间，并设置现在锁的过期时间
    String oldValueStr = jedis.getSet(key_resource_id, expiresStr);

    if (oldValueStr != null && oldValueStr.equals(currentValueStr)) {
        // 考虑多线程并发的情况，只有一个线程的设置值和当前值相同，它才可以加锁
        return true;
    }
}

//其他情况，均返回加锁失败
return false;
```

- - 过期时间是客户端生成的，在分布式环境中需要保障每个客户端的时间同步
  - jedis.getSet 可能出现锁覆盖的情况

- 方案三：使用Lua脚本(包含SETNX + EXPIRE两条指令)

```java
String lua_scripts = "if redis.call('setnx',KEYS[1],ARGV[1]) == 1 then" +
    " redis.call('expire',KEYS[1],ARGV[2]) return 1 else return 0 end";

Object result = jedis.eval(lua_scripts, Collections.singletonList(key_resource_id), Collections.singletonList(values));
//判断是否成功
return result.equals(1L);
```

- 方案四：SET的扩展命令（SET EX PX NX）

- - **SET key value [EX seconds] [PX milliseconds] [NX|XX]**
  - **NX：**表示key不存在的时候，才能set成功，也即保证只有第一个客户端请求才能获得锁，而其他客户端请求只能等其释放锁，才能获取
  - **EX seconds：**设定key的过期时间，时间单位是秒
  - **PX milliseconds：**设定key的过期时间，单位为毫秒
  - **XX：**仅当key存在时设置值

```java
@RequestMapping(value = "/test")
public String test(){
    String lockKey = "product_001";

    try{
        Boolean result = stringRedisTemplate.opsForValue()
            .setIfAbsent(lockKey, "testlock", 10, TimeUnit.SECONDS);
        if(!result){
            // 代表已经加锁了
            return "error_code";
        }
        // 从redis 中拿当前库存的值
        int stock = Integer.parseInt(stringRedisTemplate.opsForValue().get("stock"));
        if(stock > 0){
            int realStock = stock - 1;
            stringRedisTemplate.opsForValue().set("stock", realStock + "");
            System.out.println("扣减成功，剩余库存：" + realStock);
        }else{
            System.out.println("扣减失败，库存不足");
        }
    } finally {
        // 释放锁
        stringRedisTemplate.delete(lockKey);
    }

    return "end";
}
```

- - 存在超时时间过短，业务代码未执行完的情况
  - 锁被其他线程误删。线程a执行完后，去释放锁。但是它不知道当前的锁可能是线程b持有的（线程a去释放锁时，有可能过期时间已经到了，此时线程b进来占有了锁）。那线程a就把线程b的锁释放掉了，但是线程b临界区业务代码可能都还没执行完呢



- 方案五：SET EX PX NX + 校验唯一随机值，再释放锁

```java
if（jedis.set(key_resource_id, uni_request_id, "NX", "EX", 100s) == 1）{ //加锁
    try {
        // 业务处理
    }catch(){
    }
    finally {
        // 判断是不是当前线程加的锁,是才释放，非原子
        if (uni_request_id.equals(jedis.get(key_resource_id))) {
            jedis.del(lockKey); // 释放锁
        }
    }
}
```

为了更严谨，一般也是用lua脚本代替

```lua
if redis.call('get',KEYS[1]) == ARGV[1] then 
  return redis.call('del',KEYS[1]) 
else
  return 0
end;
```



- 方案六：单机分布式锁 Redisson



- 方案七：多机分布式锁 Redlock + Redisson



### 单机Redisson分布式锁

#### 引入依赖

```xml
<dependency>
  <groupId>org.redisson</groupId>
  <artifactId>redisson</artifactId>
  <version>xxx</version>
</dependency>
```

#### 初始化客户端

```java
@Bean
public RedissonClient redisson(){
    // 单机模式
    Config config = new Config();
    config.useSingleServer().setAddress("redis://127.0.0.1:6379").setDatabase(0);
    return Redisson.create(config);
}
```

#### 实现分布式锁

```java
@RestController
public class IndexController {

    @Autowired
    private RedissonClient redisson;

    @Autowired
    private StringRedisTemplate stringRedisTemplate;

    /**
     * 模拟下单减库存的场景
     * 
     * @return
     */
    @RequestMapping(value = "/test")
    public String test() {
        String lockKey = "product_001";
        // 1.获取锁对象
        RLock redissonLock = redisson.getLock(lockKey);
        try {
            // 2.加锁  等价于 setIfAbsent(lockKey,"testlock",10,TimeUnit.SECONDS);
            redissonLock.lock();
            // 从redis 中拿当前库存的值
            int stock = Integer.parseInt(stringRedisTemplate.opsForValue().get("stock"));
            if (stock > 0) {
                int realStock = stock - 1;
                stringRedisTemplate.opsForValue().set("stock", realStock + "");
                System.out.println("扣减成功，剩余库存：" + realStock);
            } else {
                System.out.println("扣减失败，库存不足");
            }
        } finally {
            // 3.释放锁
            redissonLock.unlock();
        }
        return "end";
    }
}
```

Redisson的lock方法最终执行的是 lua 脚本

```lua
-- exists 判断 key 是否存在
if (redis.call('exists', KEYS[1]) == 0) then
    -- 设置 key
    redis.call('hset', KEYS[1], ARGV[2], 1);
    -- 追加过期时间
    redis.call('pexpire', KEYS[1], ARGV[1]); 
    return nil; 
end;
```

lua 脚本在 redis 中执行时，会被当做一条命令，所以能够保证原子性，Redisson中大量使用了lua脚本



### 多机分布式锁 Redlock + Redisson

**org.redisson.RedissonMultiLock** 对象可实现 Redlock 锁算法，将多个 RLock 对象划分为一组并且将它们当做一个锁来处理，每个 RLock 对象可以属于不同的 Redisson 实例

```java
RLock lock1 = redissonInstance1.getLock("lock1");
RLock lock2 = redissonInstance2.getLock("lock2");
RLock lock3 = redissonInstance3.getLock("lock3");


RedissonMultiLock lock = new RedissonMultiLock(lock1, lock2, lock3);
// locks: lock1 lock2 lock3
lock.lock();
...
lock.unlock();
```



**RedissonRedLock extends** **RedissonMultiLock**

```java
Config config1 = new Config();
config1.useSingleServer().setAddress("redis://172.0.0.1:5378").setDatabase(0);
RedissonClient redissonClient1 = Redisson.create(config1);

Config config2 = new Config();
config2.useSingleServer().setAddress("redis://172.0.0.1:5379").setDatabase(0);
RedissonClient redissonClient2 = Redisson.create(config2);

Config config3 = new Config();
config3.useSingleServer().setAddress("redis://172.0.0.1:5380").setDatabase(0);
RedissonClient redissonClient3 = Redisson.create(config3);

/**
* 获取多个 RLock 对象
*/
RLock lock1 = redissonInstance1.getLock("lockKey");
RLock lock2 = redissonInstance2.getLock("lockKey");
RLock lock3 = redissonInstance3.getLock("lockKey");

/**
* 根据多个 RLock 对象构建 RedissonRedLock
*/
RedissonRedLock lock = new RedissonRedLock(lock1, lock2, lock3);

try {
    /**
    * 尝试获取锁
    * waitTimeout 尝试获取锁的最大等待时间，超过这个值，则认为获取锁失败
    * leaseTime   锁的持有时间，超过这个时间锁会自动失效（值应设置为大于业务处理的时间，确保在锁有效期内业务能处理完）
    */
    boolean res = redLock.tryLock((long)waitTimeout, (long)leaseTime, TimeUnit.SECONDS);
    if (res) {
        //成功获得锁，在这里处理业务
    }
} catch (Exception e) {
    throw new RuntimeException("aquire lock fail");
} finally {
    //解锁
    redLock.unlock();
}
```

## Zookeeper分布式锁

ZooKeeper实现的分布式锁，性能并不太高，在高性能高并发的场景下不建议使用分布式锁

### 三种方案

- 临时节点方案：缺点是每次竞争锁，都只有一个线程拿到锁，会导致大量线程同时阻塞抢锁，性能低甚至宕机
- 临时顺序节点方案：临时顺序节点与临时节点不同的是产生的节点是有序的
- Curator分布式锁工具中的共享可重入锁：全局同步锁，在同一时间不会有两个客户端持有一个锁

### 临时顺序节点方案

**排队取号**

**公平可重入锁**，**Zookeeper临时顺序节点**，递增有序性，可以确保锁的公平，节点监听机制，可以保障占有锁的传递有序而且高效

分布式锁的算法

- 一把分布式锁通常使用一个Znode节点表示；如果锁对应的Znode节点不存在，首先创建Znode节点。这里假设为“/test/lock”，代表了一把需要创建的分布式锁
- 抢占锁的所有客户端，使用锁的Znode节点的子节点列表来表示；如果某个客户端需要占用锁，则在“/test/lock”下创建一个临时有序的子节点。所有临时有序子节点，尽量共用一个有意义的子节点前缀。比如“/test/lock/seq-”
- 判定客户端是否占有锁，客户端创建子节点后，需要进行判断：自己创建的子节点，是否为当前子节点列表中序号最小的子节点。如果是，则认为加锁成功；如果不是，则监听前一个Znode子节点变更消息，等待前一个节点释放锁。
- 一旦队列中的后面的节点，获得前一个子节点变更通知，则开始进行判断，判断自己是否为当前子节点列表中序号最小的子节点，如果是，则认为加锁成功；如果不是，则持续监听，一直到获得锁。
- 获取锁后，开始处理业务流程。完成业务流程后，删除自己的对应的子节点，完成释放锁的工作，以方面后继节点能捕获到节点变更通知，获得分布式锁。

#### Lock接口

定义了一个锁的接口Lock，两个抽象方法：一个加锁方法，一个解锁方法

```java
public interface Lock {
    /**
     * 加锁方法
     *
     * @return 是否成功加锁
     */
    boolean lock() throws Exception;

    /**
     * 解锁方法
     *
     * @return 是否成功解锁
     */
    boolean unlock();
}
```

#### 锁实现

**lock()**

- 首先尝试着去加锁，如果加锁失败就去等待，然后再重复

```java
public class ZkLock implements Lock {
    //ZkLock的节点链接
    private static final String ZK_PATH = "/test/lock";
    private static final String LOCK_PREFIX = ZK_PATH + "/";
    private static final long WAIT_TIME = 1000;
    
    //Zk客户端
    CuratorFramework client = null;

    private String locked_short_path = null;
    private String locked_path = null;
    private String prior_path = null;
    final AtomicInteger lockCount = new AtomicInteger(0);
    
    private Thread thread;

    public ZkLock() {
        ZKclient.instance.init();
        synchronized (ZKclient.instance) {
            if (!ZKclient.instance.isNodeExist(ZK_PATH)) {
                ZKclient.instance.createNode(ZK_PATH, null);
            }
        }
        client = ZKclient.instance.getClient();
    }

    @Override
    public boolean lock() {
		//可重入，确保同一线程，可以重复加锁
        synchronized (this) {
            if (lockCount.get() == 0) {
                thread = Thread.currentThread();
                lockCount.incrementAndGet();
            } else {
                if (!thread.equals(Thread.currentThread())) {
                    return false;
                }
                lockCount.incrementAndGet();
                return true;
            }
        }

        try {
            boolean locked = false;
			//首先尝试着去加锁
            locked = tryLock();

            if (locked) {
                return true;
            }
            //如果加锁失败就去等待
            while (!locked) {

                await();

                //获取等待的子节点列表
                List<String> waiters = getWaiters();
				//判断，是否加锁成功
                if (checkLocked(waiters)) {
                    locked = true;
                }
            }
            return true;
        } catch (Exception e) {
            e.printStackTrace();
            unlock();
        }

        return false;
    }
}
```

**tryLock()**

- 创建临时顺序节点，并且保存自己的节点路径
- 判断是否是第一个，如果是第一个，则加锁成功。如果不是，就找到前一个Znode节点，并且保存其路径到prior_path

```java
/**
 * 尝试加锁
 * @return 是否加锁成功
 * @throws Exception 异常
 */
private boolean tryLock() throws Exception {
    //创建临时Znode
    locked_path = ZKclient.instance
            .createEphemeralSeqNode(LOCK_PREFIX);
    //然后获取所有节点
    List<String> waiters = getWaiters();

    if (null == locked_path) {
        throw new Exception("zk error");
    }
    //取得加锁的排队编号
    locked_short_path = getShortPath(locked_path);

    //获取等待的子节点列表，判断自己是否第一个
    if (checkLocked(waiters)) {
        return true;
    }

    // 判断自己排第几个
    int index = Collections.binarySearch(waiters, locked_short_path);
    if (index < 0) { // 网络抖动，获取到的子节点列表里可能已经没有自己了
        throw new Exception("节点没有找到: " + locked_short_path);
    }

    //如果自己没有获得锁，则要监听前一个节点
    prior_path = ZK_PATH + "/" + waiters.get(index - 1);

    return false;
}

private String getShortPath(String locked_path) {

    int index = locked_path.lastIndexOf(ZK_PATH + "/");
    if (index >= 0) {
        index += ZK_PATH.length() + 1;
        return index <= locked_path.length() ? locked_path.substring(index) : "";
    }
    return null;
}
```

**checkLocked()**

- 判断是否可以持有锁，当前创建的节点，是否在上一步获取到的子节点列表的第一个位置

```java
private boolean checkLocked(List<String> waiters) {

    //节点按照编号，升序排列
    Collections.sort(waiters);

    // 如果是第一个，代表自己已经获得了锁
    if (locked_short_path.equals(waiters.get(0))) {
        log.info("成功的获取分布式锁,节点为{}", locked_short_path);
        return true;
    }
    return false;
}
```

**await()**

- 监听前一个ZNode节点（prior_path成员）的删除事件，可以使用两种监听方式
- Watcher 订阅
- TreeCache 订阅

```java
private void await() throws Exception {

    if (null == prior_path) {
        throw new Exception("prior_path error");
    }

    final CountDownLatch latch = new CountDownLatch(1);

    //订阅比自己次小顺序节点的删除事件
    Watcher w = new Watcher() {
        @Override
        public void process(WatchedEvent watchedEvent) {
            System.out.println("监听到的变化 watchedEvent = " + watchedEvent);
            log.info("[WatchedEvent]节点删除");

            latch.countDown();
        }
    };

    client.getData().usingWatcher(w).forPath(prior_path);
	
/*
    //订阅比自己次小顺序节点的删除事件
    TreeCache treeCache = new TreeCache(client, prior_path);
    TreeCacheListener l = new TreeCacheListener() {
        @Override
        public void childEvent(CuratorFramework client,
                                TreeCacheEvent event) throws Exception {
            ChildData data = event.getData();
            if (data != null) {
                switch (event.getType()) {
                    case NODE_REMOVED:
                        log.debug("[TreeCache]节点删除, path={}, data={}",
                                data.getPath(), data.getData());

                        latch.countDown();
                        break;
                    default:
                        break;
                }
            }
        }
    };

    treeCache.getListenable().addListener(l);
    treeCache.start();
*/
    latch.await(WAIT_TIME, TimeUnit.SECONDS);
}
```

**可重入**

只需要保障同一个线程进入加锁的代码，可以重复加锁成功即可。在lock方法前面加上可重入的判断逻辑

```java
@Override
public boolean lock() {

    //可重入的判断
    synchronized (this) {
        if (lockCount.get() == 0) {
            thread = Thread.currentThread();
            lockCount.incrementAndGet();
        } else {
            if (!thread.equals(Thread.currentThread())) {
                return false;
            }
            lockCount.incrementAndGet();
            return true;
        }
    }
}
```

为了变成可重入，在代码中增加了一个加锁的计数器lockCount，计算重复加锁的次数。如果是同一个线程加锁，只需要增加次数，直接返回，表示加锁成功。

#### 解锁

**unLock()**，表示释放锁，释放锁主要有两个工作

- 减少重入锁的计数，如果最终的值不是0，直接返回，表示成功的释放了一次
- 如果计数器为0，移除Watchers监听器，并且删除创建的Znode临时节点

```java
/**
 * 释放锁
 *
 * @return 是否成功释放锁
 */
@Override
public boolean unlock() {
	//只有加锁的线程，能够解锁
    if (!thread.equals(Thread.currentThread())) {
        return false;
    }
	//减少可重入的计数
    int newLockCount = lockCount.decrementAndGet();
	//计数不能小于0
    if (newLockCount < 0) {
        throw new IllegalMonitorStateException("Lock count has gone negative for lock: " + locked_path);
    }
	//如果计数不为0，直接返回
    if (newLockCount != 0) {
        return true;
    }
    //删除临时节点
    try {
        if (ZKclient.instance.isNodeExist(locked_path)) {
            client.delete().forPath(locked_path);
        }
    } catch (Exception e) {
        e.printStackTrace();
        return false;
    }

    return true;
}
```

#### 测试

```java
prviate int count = 0;

@Test
public void testLock() throws InterruptedException {
    for (int i = 0; i < 10; i++) {
        FutureTaskScheduler.add(() -> {
            //创建锁
            ZkLock lock = new ZkLock();
            lock.lock();
//每条线程，执行10次累加
            for (int j = 0; j < 10; j++) {
//公共的资源变量累加
                count++;
            }
            try {
                Thread.sleep(1000);
            } catch (InterruptedException e) {
                e.printStackTrace();
            }
            log.info("count = " + count);
            //释放锁
            lock.unlock();

        });
    }

    Thread.sleep(Integer.MAX_VALUE);
}
```

### Curator分布式锁InterProcessMutex

```java
prviate int count = 0;

@Test
public void testzkMutex() throws InterruptedException {

    CuratorFramework client = ZKclient.instance.getClient();
    final InterProcessMutex zkMutex = new InterProcessMutex(client, "/mutex");

    for (int i = 0; i < 10; i++) {
        FutureTaskScheduler.add(() -> {

            try {
                //获取互斥锁
                zkMutex.acquire();

                for (int j = 0; j < 10; j++) {
					//公共的资源变量累加
                    count++;
                }
                try {
                    Thread.sleep(1000);
                } catch (InterruptedException e) {
                    e.printStackTrace();
                }
                log.info("count = " + count);
                //释放互斥锁
                zkMutex.release();

            } catch (Exception e) {
                e.printStackTrace();
            }

        });
    }

    Thread.sleep(Integer.MAX_VALUE);
}
```

## etcd分布式锁

etcd 是一个分布式键值对存储，设计用来可靠而快速的保存关键数据并提供访问，etcd分布式锁并不是etcd server对外提供一个功能api，而是基于etcd的各种特性（lease、watch、mvcc等）集成的一个工具。

## Consul 分布式锁

Consul的分布式锁主要利用Key/Value存储API中的acquire和release操作来实现。acquire和release操作是类似Check-And-Set的操作