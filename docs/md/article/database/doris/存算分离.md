Doris 3.0 开始支持存算分离

## 存算一体 VS 存算分离

Doris 的整体架构由两类进程组成：Frontend (FE) 和 Backend (BE)。其中 FE 主要负责用户请求的接入、查询解析规划、元数据的管理、节点管理相关工作；BE 主要负责数据存储、查询计划的执行

### 存算一体

在存算一体架构下，BE 节点上存储与计算紧密耦合，数据主要存储在 BE 节点上，多 BE 节点采用 MPP 分布式计算架构

### 存算分离

BE 节点不再存储主数据，而是将共享存储层作为统一的数据主存储空间。同时，为了应对底层对象存储系统性能不佳和网络传输带来的性能下降，Doris 引入计算节点本地高速缓存

**元数据层：**

FE 主要存放库表元数据，Job 以及权限等 MySQL 协议依赖的信息。

Meta Service 是 Doris 存算分离元数据服务，主要负责处理导入事务，Tablet Meta，Rowset Meta 以及集群资源管理。这是一个可以横向扩展的无状态服务。

**计算层：**

存算分离模式下的 BE 是无状态的 Doris BE 节点，BE 上会缓存一部分 Tablet 元数据和数据以提高查询性能。

计算集群（Compute Cluster）是无状态的 BE 节点组成的计算资源集合，多个计算集群共享一份数据，计算集群可以随时弹性加减节点。

备注

存算分离文档中的“计算集群”概念有别于 Doris【集群部署】以及后文【创建集群】中的“集群”概念。存算分离文档中提及的“计算集群”特指在 Doris 存算分离模式下，由无状态 BE 节点组成的计算资源集合，而非【集群部署】和【创建集群】中所指的由多个 Apache Doris 节点组成的完整分布式系统。

**共享存储层：**

共享存储主要存放数据文件，包括 Segment 文件、反向索引的索引文件等。

## 如何选择

### 存算一体的优点

- 部署简易：Apache Doris 不需要依赖类似外部共享文件系统或者对象存储，仅依赖物理服务器部署 FE 和 BE 两个进程即可完成集群的搭建，可以从一个节点扩展到数百个节点，同时也增强了系统的稳定性。
- 性能优异：Apache Doris 执行计算时，计算节点可直接访问本地存储数据，充分利用机器的 IO、减少不必要的网络开销、获得更极致的查询性能。

### **存算一体的**适用场景

- 简单使用/快速试用 Doris，或在开发和测试环境中使用；
- 不具备可靠的共享存储，如 HDFS、Ceph、对象存储等；
- 业务线独立维护 Apache Doris，无专职 DBA 来维护 Doris 集群；
- 不需极致弹性扩缩容，不需 K8s 容器化，不需运行在公有云或者私有云上。

### 存算分离的优点

- 弹性的计算资源：不同时间点使用不同规模的计算资源服务业务请求，按需使用计算资源，节约成本。
- 负载（完全）隔离：不同业务之间可在共享数据的基础上隔离计算资源，兼具稳定性和高效率。
- 低存储成本：可以使用更低成本的对象存储，HDFS 等低成本存储。

### **存算分离的**适用场景

- 已使用公有云服务
- 具备可靠的共享存储系统，比如 HDFS、Ceph、对象存储等
- 需要极致的弹性扩缩容，需要 K8S 容器化，需要运行在私有云上
- 有专职团队维护整个公司的数据仓库平台



# Doris 存算分离模式部署准备

## 1. 概述

本文档介绍了 Apache Doris 存算分离模式的部署准备工作。存算分离架构旨在提高系统的可扩展性和性能，适用于大规模数据处理场景。

## 2. 架构组件

Doris 存算分离架构包含三个主要模块：

1. **Frontend (FE)**：处理用户请求和管理元数据。
2. **Backend (BE)**：无状态计算节点，执行查询任务。
3. **Meta Service (MS)**：管理元数据操作和数据回收。

## 3. 系统要求

### 3.1 硬件要求

- 最小配置：3 台服务器
- 推荐配置：5 台或更多服务器

### 3.2 软件依赖

- FoundationDB (FDB) 7.1.38 或更高版本
- OpenJDK 17

## 4. 部署规划

### 4.1 测试环境部署

单机部署所有模块，不适用于生产环境。

### 4.2 生产部署

- 3 台或更多机器部署 FDB
- 3 台或更多机器部署 FE 和 Meta Service
- 3 台或更多机器部署 BE

机器配置高时，可以考虑 FDB、FE 和 Meta Service 混布，但是磁盘不要混用。

## 5. 安装步骤

### 5.1 安装 FoundationDB

本节提供了脚本 `fdb_vars.sh` 和 `fdb_ctl.sh` 配置、部署和启动 FDB（FoundationDB）服务的分步指南。您可以下载 [doris tools](http://apache-doris-releases.oss-accelerate.aliyuncs.com/apache-doris-3.0.2-tools.tar.gz) 并从 `fdb` 目录获取 `fdb_vars.sh` 和 `fdb_ctl.sh`。

#### 5.1.1 机器要求

通常，至少需要 3 台配备 SSD 的机器来形成具有双数据副本并允许单机故障的 FoundationDB 集群。

提示

如果仅用于开发/测试目的，单台机器就足够了。

#### 5.1.2 `fdb_vars.sh` 配置

##### 必需的自定义设置

| 参数               | 描述                             | 类型                           | 示例                                                         | 注意事项                                                     |
| ------------------ | -------------------------------- | ------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| `DATA_DIRS`        | 指定 FoundationDB 存储的数据目录 | 以逗号分隔的绝对路径列表       | `/mnt/foundationdb/data1,/mnt/foundationdb/data2,/mnt/foundationdb/data3` | - 运行脚本前确保目录已创建 - 生产环境建议使用 SSD 和独立目录 |
| `FDB_CLUSTER_IPS`  | 定义集群 IP                      | 字符串（以逗号分隔的 IP 地址） | `172.200.0.2,172.200.0.3,172.200.0.4`                        | - 生产集群至少应有 3 个 IP 地址 - 第一个 IP 地址将用作协调器 - 为高可用性，将机器放置在不同机架上 |
| `FDB_HOME`         | 定义 FoundationDB 主目录         | 绝对路径                       | `/fdbhome`                                                   | - 默认路径为 /fdbhome - 确保此路径是绝对路径                 |
| `FDB_CLUSTER_ID`   | 定义集群 ID                      | 字符串                         | `SAQESzbh`                                                   | - 每个集群的 ID 必须唯一 - 可使用 `mktemp -u XXXXXXXX` 生成  |
| `FDB_CLUSTER_DESC` | 定义 FDB 集群的描述              | 字符串                         | `dorisfdb`                                                   | - 建议更改为对部署有意义的内容                               |

##### 可选的自定义设置

| 参数              | 描述                               | 类型 | 示例                 | 注意事项                                         |
| ----------------- | ---------------------------------- | ---- | -------------------- | ------------------------------------------------ |
| `MEMORY_LIMIT_GB` | 定义 FDB 进程的内存限制，单位为 GB | 整数 | `MEMORY_LIMIT_GB=16` | 根据可用内存资源和 FDB 进程的要求调整此值        |
| `CPU_CORES_LIMIT` | 定义 FDB 进程的 CPU 核心限制       | 整数 | `CPU_CORES_LIMIT=8`  | 根据可用的 CPU 核心数量和 FDB 进程的要求设置此值 |

#### 5.1.3 部署 FDB 集群

使用 `fdb_vars.sh` 配置环境后，您可以在每个节点上使用 `fdb_ctl.sh` 脚本部署 FDB 集群。

```bash
./fdb_ctl.sh deploy
```



此命令启动 FDB 集群的部署过程。

#### 5.1.4 启动 FDB 服务

FDB 集群部署完成后，您可以使用 `fdb_ctl.sh` 脚本启动 FDB 服务。

```bash
./fdb_ctl.sh start
```



此命令启动 FDB 服务，使集群工作并获取 FDB 集群连接字符串，后续可以用于配置 MetaService。

### 5.2 安装 OpenJDK 17

1. 下载 [OpenJDK 17](https://download.java.net/java/GA/jdk17.0.1/2a2082e5a09d4267845be086888add4f/12/GPL/openjdk-17.0.1_linux-x64_bin.tar.gz)
2. 解压并设置环境变量 JAVA_HOME.

### 5.3 安装 S3 或 HDFS 服务（可选）

Apache Doris 存算分离模式会将数据存储在 S3 服务或 HDFS 服务上面，如果您已经有相关服务，直接使用即可。如果没有，本文档提供 MinIO 的简单部署教程：

1. 在 MinIO 的[下载页面](https://min.io/download?license=agpl&platform=linux)选择合适的版本以及操作系统，下载对应的 Server 以及 Client 的二进制包或安装包。

2. 启动 MinIO Server

   ```bash
   export MINIO_REGION_NAME=us-east-1
   export MINIO_ROOT_USER=minio # 在较老版本中，该配置为 MINIO_ACCESS_KEY=minio
   export MINIO_ROOT_PASSWORD=minioadmin # 在较老版本中，该配置为 MINIO_SECRET_KEY=minioadmin
   nohup ./minio server /mnt/data 2>&1 &
   ```

   

3. 配置 MinIO Client

   ```bash
   # 如果你使用的是安装包安装的客户端，那么客户端名为 mcli，直接下载客户端二进制包，则其名为 mc
   ./mc config host add myminio http://127.0.0.1:9000 minio minioadmin
   ```

   

4. 创建一个桶

   ```bash
   ./mc mb myminio/doris
   ```

   

5. 验证是否正常工作

   ```bash
   # 上传一个文件
   ./mc mv test_file myminio/doris
   # 查看这个文件
   ./mc ls myminio/doris
   ```

   

## 7. 注意事项

- 确保所有节点的时间同步
- 定期备份 FoundationDB 数据
- 根据实际负载调整 FoundationDB 和 Doris 的配置参数



# 编译部署



## 2. 获取二进制

### 2.1 直接下载

已编译好的二进制文件（包含所有 Doris 模块）可从 [Doris 下载页面](https://doris.apache.org/download/) 获取（选择 3.0.2 或更高版本）。

### 2.2 编译产出 (可选)

使用代码库自带的 `build.sh` 脚本进行编译。新增的 MS 模块通过 `--cloud` 参数编译。

```shell
sh build.sh --fe --be --cloud 
```



编译完成后，在 `output` 目录下会新增 `ms` 目录：

```text
output
├── be
├── fe
└── ms
    ├── bin
    ├── conf
    └── lib
```



## 3. Meta Service 部署

### 3.1 配置

在 `./conf/doris_cloud.conf` 文件中，主要需要修改以下两个参数：

1. `brpc_listen_port`：Meta Service 的监听端口，默认为 5000。
2. `fdb_cluster`：FoundationDB 集群的连接信息，部署 FoundationDB 时可以获取。(如果使用 Doris 提供的 fdb_ctl.sh 部署的话，可在 `$FDB_HOME/conf/fdb.cluster` 文件里获取该值)。

示例配置：

```shell
brpc_listen_port = 5000
fdb_cluster = xxx:yyy@127.0.0.1:4500
```



注意：`fdb_cluster` 的值应与 FoundationDB 部署机器上的 `/etc/foundationdb/fdb.cluster` 文件内容一致 (如果使用 Doris 提供的 fdb_ctl.sh 部署的话，可在 `$FDB_HOME/conf/fdb.cluster` 文件里获取该值)。

**示例，文件的最后一行就是要填到 doris_cloud.conf 里 fdb_cluster 字段的值**

```shell
cat /etc/foundationdb/fdb.cluster

# DO NOT EDIT!
# This file is auto-generated, it is not to be edited by hand.
cloud_ssb:A83c8Y1S3ZbqHLL4P4HHNTTw0A83CuHj@127.0.0.1:4500
```



### 3.2 启动与停止

*环境要求*

确保已正确设置 `JAVA_HOME` 环境变量，指向 OpenJDK 17，进入 `ms` 目录。

*启动命令*

```shell
export JAVA_HOME=${path_to_jdk_17}
bin/start.sh --daemon
```



```text
LIBHDFS3_CONF=
starts doris_cloud with args: --meta-service
wait and check doris_cloud start successfully
successfully started brpc listening on port=5000 time_elapsed_ms=11
doris_cloud start successfully
```



启动脚本返回值为 0 表示启动成功，否则启动失败。

信息

在 3.0.4 中，启动脚本会输出更多信息：

```text
2024-12-26 15:31:53 start with args: --meta-service
wait and check MetaService and Recycler start successfully
process working directory: "/mnt/disk1/doris/ms"
pid=1666015 written to file=./bin/doris_cloud.pid
version:{doris-3.0.4-release} code_version:{commit=fd44740fadabebfedb5da201d7ce427a5dd47c44 time=2025-01-16 18:53:00 +0800} build_info: ...

MetaService has been started successfully
successfully started service listening on port=5000 time_elapsed_ms=19
```



*停止命令*

```shell
bin/stop.sh
```



生产环境中请确保至少有 3 个 Meta Service 节点。

## 4. 数据回收功能独立部署（可选）

信息

Meta Service 本身具备了元数据管理和回收功能，这两个功能可以独立部署，如果你想独立部署，可以参考这一节。

*准备工作*

1. 创建新的工作目录（如 `recycler`）。

2. 复制 `ms` 目录内容到新目录：

   ```shell
   cp -r ms recycler
   ```

   

*配置*

在新目录的配置文件中修改 BRPC 监听端口 `brpc_listen_port` 和 `fdb_cluster` 的值。

*启动数据回收功能*

```shell
export JAVA_HOME=${path_to_jdk_17}
bin/start.sh --recycler --daemon
```



*启动仅元数据操作功能*

```shell
export JAVA_HOME=${path_to_jdk_17}
bin/start.sh --meta-service --daemon
```



## 5. FE 和 BE 的启动流程

本节详细说明了在存算分离架构下启动 FE（Frontend）和 BE（Backend）的步骤。

### 5.1 启动顺序

1. 以 MASTER 角色启动实例的第一个 FE
2. 向实例中添加其他 FE 和 BE
3. 添加第一个 Storage Vault

### 5.2 启动 MASTER 角色的 FE

#### 5.2.1 配置 fe.conf

在 `fe.conf` 文件中，需要配置以下关键参数：

1. `deploy_mode`

   - 描述：指定 doris 启动模式
   - 格式：cloud 表示存算分离模式，其它存算一体模式
   - 示例：`cloud`

2. `cluster_id`

   - 描述：存算分离架构下集群的唯一标识符，不同的集群必须设置不同的 cluster_id

   - 格式：int 类型

   - 示例：可以使用如下 shell 脚本生成一个随机 id 使用。

     ```shell
     echo $(($((RANDOM << 15)) | $RANDOM))
     ```

     

     警告

     **不同的集群必须设置不同的 cluster_id**

3. `meta_service_endpoint`

   - 描述：Meta Service 的地址和端口
   - 格式：`IP地址:端口号`
   - 示例：`127.0.0.1:5000`, 可以用逗号分割配置多个 meta service。

#### 5.2.2 启动 FE

启动命令示例：

```bash
bin/start_fe.sh --daemon
```



第一个 FE 进程初始化集群并以 FOLLOWER 角色工作。使用 mysql 客户端连接 FE 使用 `show frontends` 确认刚才启动的 FE 是 master。

### 5.3 添加其他 FE 节点

其他节点同样根据上述步骤修改配置文件并启动，使用 mysql 客户端连接 Master 角色的 FE，并用以下 SQL 命令添加额外的 FE 节点：

```sql
ALTER SYSTEM ADD FOLLOWER "host:port";
```



将 `host:port` 替换为 FE 节点的实际地址和编辑日志端口。更多信息请参见 [ADD FOLLOWER](https://doris.apache.org/zh-CN/docs/3.0/sql-manual/sql-statements/cluster-management/instance-management/ADD-FOLLOWER) 和 [ADD OBSERVER](https://doris.apache.org/zh-CN/docs/3.0/sql-manual/sql-statements/cluster-management/instance-management/ADD-OBSERVER)。

生产环境中请确保在 FOLLOWER 角色中的前端 (FE) 节点总数，包括第一个 FE，保持为奇数。一般来说，三个 FOLLOWER 就足够了。OBSERVER 角色的前端节点可以是任意数量。

### 5.4 添加 BE 节点

要向集群添加 Backend 节点，请对每个 Backend 执行以下步骤：

#### 5.4.1 配置 be.conf

在 `be.conf` 文件中，需要配置以下关键参数：

1. `deploy_mode`
   - 描述：指定 doris 启动模式
   - 格式：cloud 表示存算分离模式，其它存算一体模式
   - 示例：`cloud`
2. `file_cache_path`
   - 描述：用于文件缓存的磁盘路径和其他参数，以数组形式表示，每个磁盘一项。`path` 指定磁盘路径，`total_size` 限制缓存的大小；-1 或 0 将使用整个磁盘空间。
   - 格式：[{"path":"/path/to/file_cache","total_size":21474836480},{"path":"/path/to/file_cache2","total_size":21474836480}]
   - 示例：[{"path":"/path/to/file_cache","total_size":21474836480},{"path":"/path/to/file_cache2","total_size":21474836480}]
   - 默认：[{"path":"${DORIS_HOME}/file_cache"}]

#### 5.4.1 启动和添加 BE

1. 启动 Backend：

   使用以下命令启动 Backend：

   ```bash
   bin/start_be.sh --daemon
   ```

   

2. 将 Backend 添加到集群：

   使用 MySQL 客户端连接到任意 Frontend，并执行：

   ```sql
   ALTER SYSTEM ADD BACKEND "<ip>:<heartbeat_service_port>" [PROTERTIES propertires];
   ```

   

   将 `<ip>` 替换为新 Backend 的 IP 地址，将 `<heartbeat_service_port>` 替换为其配置的心跳服务端口（默认为 9050）。

   可以通过 PROPERTIES 设置 BE 所在的 计算组。

   更详细的用法请参考 [ADD BACKEND](https://doris.apache.org/zh-CN/docs/3.0/sql-manual/sql-statements/cluster-management/instance-management/ADD-BACKEND) 和 [REMOVE BACKEND](https://doris.apache.org/zh-CN/docs/3.0/sql-manual/sql-statements/cluster-management/instance-management/DROP-BACKEND)。

3. 验证 Backend 状态：

   检查 Backend 日志文件（`be.log`）以确保它已成功启动并加入集群。

   您还可以使用以下 SQL 命令检查 Backend 状态：

   ```sql
   SHOW BACKENDS;
   ```

   

   这将显示集群中所有 Backend 及其当前状态。

## 6. 创建 Storage Vault

Storage Vault 是 Doris 存算分离架构中的重要组件。它们代表了存储数据的共享存储层。您可以使用 HDFS 或兼容 S3 的对象存储创建一个或多个 Storage Vault。可以将一个 Storage Vault 设置为默认 Storage Vault，系统表和未指定 Storage Vault 的表都将存储在这个默认 Storage Vault 中。默认 Storage Vault 不能被删除。以下是为您的 Doris 集群创建 Storage Vault 的方法：

### 6.1 创建 HDFS Storage Vault

要使用 SQL 创建 Storage Vault，请使用 MySQL 客户端连接到您的 Doris 集群

```sql
CREATE STORAGE VAULT IF NOT EXISTS hdfs_vault
    PROPERTIES (
    "type"="hdfs",
    "fs.defaultFS"="hdfs://127.0.0.1:8020"
    );
```



### 6.2 创建 S3 Storage Vault

要使用兼容 S3 的对象存储创建 Storage Vault，请按照以下步骤操作：

1. 使用 MySQL 客户端连接到您的 Doris 集群。
2. 执行以下 SQL 命令来创建 S3 Storage Vault：

```sql
CREATE STORAGE VAULT IF NOT EXISTS s3_vault
    PROPERTIES (
    "type"="S3",
    "s3.endpoint"="s3.us-east-1.amazonaws.com",
    "s3.access_key" = "ak",
    "s3.secret_key" = "sk",
    "s3.region" = "us-east-1",
    "s3.root.path" = "ssb_sf1_p2_s3",
    "s3.bucket" = "doris-build-1308700295",
    "provider" = "S3"
    );
```



要在其他对象存储上创建 Storage Vault，请参考 [创建 Storage Vault ](https://doris.apache.org/zh-CN/docs/3.0/sql-manual/sql-statements/cluster-management/storage-management/CREATE-STORAGE-VAULT)。

### 6.3 设置默认 Storage Vault

使用如下 SQL 语句设置一个默认 Storage Vault。

```sql
SET <storage_vault_name> AS DEFAULT STORAGE VAULT
```



## 7. 注意事项

- 仅元数据操作功能的 Meta Service 进程应作为 FE 和 BE 的 `meta_service_endpoint` 配置目标。
- 数据回收功能进程不应作为 `meta_service_endpoint` 配置目标。

[
](https://github.com/apache/doris-website/tree/master/i18n/zh-CN/docusaurus-plugin-content-docs/version-3.0/compute-storage-decoupled/compilation-and-deployment.md)



# 管理 Storage Vault

Storage Vault 是 Doris 在存算分离模式中所使用的远程共享存储，可配置一个或多个 Storage Vault，可将不同表存储在不同 Storage Vault 上。

## 创建 Storage Vault

**语法**

```sql
CREATE STORAGE VAULT [IF NOT EXISTS] <vault_name>
PROPERTIES
("key" = "value",...)
```



<vault_name> 是用户定义的 Storage Vault 名称，是用户接口用于访问 Storage Vault 的标识。

### 创建 HDFS Storage Vault

创建基于 HDFS 的存算分离模式 Doris 集群，需要确保所有的节点 (包括 FE / BE 节点、Meta Service) 均有权限访问所指定的 HDFS，包括提前完成机器的 Kerberos 授权配置和连通性检查（可在对应的每个节点上使用 Hadoop Client 进行测试）等。

```sql
CREATE STORAGE VAULT IF NOT EXISTS hdfs_vault_demo
PROPERTIES (
    "type" = "hdfs",                                     -- required
    "fs.defaultFS" = "hdfs://127.0.0.1:8020",            -- required
    "path_prefix" = "big/data",                          -- optional,  一般按照业务名称填写
    "hadoop.username" = "user"                           -- optional
    "hadoop.security.authentication" = "kerberos"        -- optional
    "hadoop.kerberos.principal" = "hadoop/127.0.0.1@XXX" -- optional
    "hadoop.kerberos.keytab" = "/etc/emr.keytab"         -- optional
);
```



### 创建 S3 Storage Vault

```sql
CREATE STORAGE VAULT IF NOT EXISTS s3_vault_demo
PROPERTIES (
    "type" = "S3",                                 -- required
    "s3.endpoint" = "oss-cn-beijing.aliyuncs.com", -- required
    "s3.region" = "cn-beijing",                    -- required
    "s3.bucket" = "bucket",                        -- required
    "s3.root.path" = "big/data/prefix",            -- required
    "s3.access_key" = "ak",                        -- required
    "s3.secret_key" = "sk",                        -- required
    "provider" = "OSS",                            -- required
    "use_path_style" = "false"                     -- optional
);
```



更多参数说明及示例可见 [CREATE-STORAGE-VAULT](https://doris.apache.org/zh-CN/docs/3.0/sql-manual/sql-statements/cluster-management/storage-management/CREATE-STORAGE-VAULT)。

**注意** 提供的对象存储路径必须具有head/get/list/put/multipartUpload/delete访问权限。

## 查看 Storage Vault

**语法**

```sql
SHOW STORAGE VAULTS
```



返回结果包含 4 列，分别为 Storage Vault 名称、Storage Vault ID、属性以及是否为默认 Storage Vault。

### 设置默认 Storage Vault

**语法**

```sql
SET <vault_name> AS DEFAULT STORAGE VAULT
```



## 建表时指定 Storage Vault

建表时在 `PROPERTIES` 中指定 `storage_vault_name`，则数据会存储在指定 `vault name` 所对应的 Storage Vault 上。建表成功后，该表不允许再修改 `storage_vault`，即不支持更换 Storage Vault。

**示例**

```sql
CREATE TABLE IF NOT EXISTS supplier (
  s_suppkey int(11) NOT NULL COMMENT "",
  s_name varchar(26) NOT NULL COMMENT "",
  s_address varchar(26) NOT NULL COMMENT "",
  s_city varchar(11) NOT NULL COMMENT "",
  s_nation varchar(16) NOT NULL COMMENT "",
  s_region varchar(13) NOT NULL COMMENT "",
  s_phone varchar(16) NOT NULL COMMENT ""
)
UNIQUE KEY (s_suppkey)
DISTRIBUTED BY HASH(s_suppkey) BUCKETS 1
PROPERTIES (
  "replication_num" = "1",
  "storage_vault_name" = "hdfs_demo_vault"
);
```



## 更改 Storage Vault

用于更新 Storage Vault 配置的可修改属性。

S3 Storage Vault 允许修改的属性：

- `VAULT_NAME`
- `s3.access_key`
- `s3.secret_key`
- `use_path_style`

HDFS Storage Vault 禁止修改的属性：

- `path_prefix`
- `fs.defaultFS`

更多属性说明见 [CREATE-STORAGE-VAULT](https://doris.apache.org/zh-CN/docs/3.0/sql-manual/sql-statements/cluster-management/storage-management/CREATE-STORAGE-VAULT)。

**示例**

```sql
ALTER STORAGE VAULT old_s3_vault
PROPERTIES (
    "type" = "S3",
    "VAULT_NAME" = "new_s3_vault",
    "s3.access_key" = "new_ak"
    "s3.secret_key" = "new_sk"
);
```



```sql
ALTER STORAGE VAULT old_hdfs_vault
PROPERTIES (
    "type" = "hdfs",
    "VAULT_NAME" = "new_hdfs_vault",
    "hadoop.username" = "hdfs"
);
```



## 删除 Storage Vault

暂不支持

## Storage Vault 权限

向指定的 MySQL 用户授予某个 Storage Vault 的使用权限，使该用户可以进行建表时指定该 Storage Vault 或查看 Storage Vault 等操作。

### 授予

```sql
GRANT
    USAGE_PRIV
    ON STORAGE VAULT <vault_name>
    TO { ROLE | USER } {<role> | <user>}
```



仅 Admin 用户有权限执行 `GRANT` 语句，该语句用于向 User / Role 授予指定 Storage Vault 的权限。拥有某个 Storage Vault 的 `USAGE_PRIV` 权限的 User / Role 可进行以下操作：

- 通过 `SHOW STORAGE VAULTS` 查看该 Storage Vault 的信息；
- 建表时在 `PROPERTIES` 中指定使用该 Storage Vault。

### 撤销

```sql
grant usage_priv on storage vault my_storage_vault to user1
```



撤销指定的 MySQL 用户的 Storage Vault 权限。

**语法**

```sql
REVOKE 
    USAGE_PRIV
    ON STORAGE VAULT <vault_name>
    FROM { ROLE | USER } {<role> | <user>}
```



仅 Admin 用户有权限执行 `REVOKE` 语句，用于撤销 User / Role 拥有的对指定 Storage Vault 的权限。

**示例**

```sql
revoke usage_priv on storage vault my_storage_vault from user1
```





# 计算组操作

在存算分离架构下，可以将一个或多个计算节点 (BE) 组成一个计算组 (Compute Group)。本文档介绍如何使用计算组，其中涉及的操作包括：

- 查看所有计算组
- 计算组授权
- 在用户级别绑定计算组 (`default_compute_group`) 以达到用户级别的隔离效果

注意

3.0.2 之前的版本中叫做计算集群（Compute Cluster）。

## 计算组使用场景

在多计算组的架构下，可以通过将一个或多个无状态的 BE 节点组成计算集群，利用计算集群指定语句 (use @<compute_group_name>) 将特定负载分配到特定的计算集群中，从而实现多导入和查询负载的物理隔离。

假设当前有两个计算集群：C1 和 C2。

- **读读隔离**：在发起两个大型查询之前，分别使用 `use @c1` 和 `use @c2`，确保两个查询在不同的计算节点上运行，从而避免在访问相同数据集时因 CPU 和内存等资源竞争而相互干扰。
- **读写隔离**：Doris 的数据导入会消耗大量资源，尤其是在大数据量和高频导入的场景中。为了避免查询和导入之间的资源竞争，可以通过 `use @c1` 和 `use @c2` 指定查询在 C1 上执行，导入在 C2 上执行。同时，C1 计算集群可以访问 C2 计算集群中新导入的数据。
- **写写隔离**：与读写隔离类似，导入之间也可以进行隔离。例如，当系统中存在高频小量导入和大批量导入时，批量导入通常耗时较长且重试成本高，而高频小量导入耗时短且重试成本低。为了避免小量导入对批量导入的干扰，可以通过 `use @c1` 和 `use @c2`，将小量导入指定到 C1 上执行，批量导入指定到 C2 上执行。

## 默认计算组的选择机制

当用户未明确[设置默认计算组](https://doris.apache.org/zh-CN/docs/3.0/compute-storage-decoupled/managing-compute-cluster#设置默认计算组)时，系统将自动为用户选择一个具有存活计算节点且用户具有使用权限的计算组。在特定会话中确定默认计算组后，默认计算组将在该会话期间保持不变，除非用户显式更改了默认设置。

在不同次的会话中，若发生以下情况，系统可能会自动更改用户的默认计算组：

- 用户失去了在上次会话中所选择默认计算组的使用权限
- 有计算组被添加或移除
- 上次所选择的默认计算组不再具有存活计算节点

其中，情况一和情况二必定会导致系统自动选择的默认计算组更改，情况三可能会导致更改。

## 查看所有计算组

使用 `SHOW COMPUTE GROUPS` 命令可以查看当前仓库中的所有计算组。返回结果会根据用户权限级别显示不同内容：

- 具有 `ADMIN` 权限的用户可以查看所有计算组
- 普通用户只能查看其拥有使用权限（USAGE_PRIV）的计算组
- 如果用户没有任何计算组的使用权限，则返回结果为空

```sql
SHOW COMPUTE GROUPS;
```



## 添加计算组

操作计算组需要具备 `OPERATOR` 权限，即节点管理权限。有关详细信息，请参阅[权限管理](https://doris.apache.org/zh-CN/docs/3.0/sql-manual/sql-statements/account-management/GRANT-TO)。默认情况下，只有 root 账号拥有 `OPERATOR` 权限，但可以通过 `GRANT` 命令将此权限授予其他账号。 要添加 BE 并为其指定计算组，请使用 [Add BE](https://doris.apache.org/zh-CN/docs/3.0/sql-manual/sql-statements/cluster-management/instance-management/ADD-BACKEND) 命令。例如：

```sql
ALTER SYSTEM ADD BACKEND 'host:9050' PROPERTIES ("tag.compute_group_name" = "new_group");
```



上面命令会将`host:9050`这台节点添加到`new_group`这个计算组中，您也可以不指定计算组，默认会添加到`default_compute_group`组里，示例：

```sql
ALTER SYSTEM ADD BACKEND 'host:9050';
```



## 授予计算组访问权限

前置条件：当前操作用户具备 `ADMIN` 权限，或者当前用户属于 admin role。

```sql
GRANT USAGE_PRIV ON COMPUTE GROUP {compute_group_name} TO {user};
```



## 撤销计算组访问权限

前置条件：当前操作用户具备 `ADMIN` 权限，或者当前用户属于 admin role。

```sql
REVOKE USAGE_PRIV ON COMPUTE GROUP {compute_group_name} FROM {user};
```



## 设置默认计算组

为当前用户设置默认计算组（此操作需要当前用户已经拥有计算组的使用权限）：

```sql
SET PROPERTY 'default_compute_group' = '{clusterName}';
```



为其他用户设置默认计算组（此操作需要 Admin 权限）：

```sql
SET PROPERTY FOR {user} 'default_compute_group' = '{clusterName}';
```



查看当前用户默认计算组，返回结果中`default_compute_group` 的值即为默认计算组：

```sql
SHOW PROPERTY;
```



查看其他用户默认计算组，此操作需要当前用户具备 admin 权限，返回结果中`default_compute_group` 的值即为默认计算组：

```sql
SHOW PROPERTY FOR {user};
```



查看当前仓库下所有可用的计算组：

```sql
SHOW COMPUTE GROUPS;
```



备注

- 若当前用户拥有 Admin 角色，例如：`CREATE USER jack IDENTIFIED BY '123456' DEFAULT ROLE "admin"`，则：
  - 可以为自身以及其他用户设置默认计算组；
  - 可以查看自身以及其他用户的 `PROPERTY`。
- 若当前用户无 Admin 角色，例如：`CREATE USER jack1 IDENTIFIED BY '123456'`，则：
  - 可以为自身设置默认计算组；
  - 可以查看自身的 `PROPERTY`；
  - 无法查看所有计算组，因该操作需要 `GRANT ADMIN` 权限。
- 若当前用户未配置默认计算组，现有系统在执行数据读写操作时将会触发错误。为解决这一问题，用户可通过执行 `use @cluster` 命令来指定当前 Context 所使用的计算组，或者使用 `SET PROPERTY` 语句来设置默认计算组。
- 若当前用户已配置默认计算组，但随后该集群被删除，则在执行数据读写操作时同样会触发错误。用户可通过执行 `use @cluster` 命令来重新指定当前 Context 所使用的计算组，或者利用 `SET PROPERTY` 语句来更新默认集群设置。

## 切换计算组

用户可在存算分离架构中指定使用的数据库和计算组。

**语法**

```sql
USE { [catalog_name.]database_name[@compute_group_name] | @compute_group_name }
```



若数据库或计算组名称包含是保留关键字，需用反引号将相应的名称 ``` 包围。

## 计算组扩缩容

通过 `ALTER SYSTEM ADD BACKEND` 以及 `ALTER SYSTEM DECOMMISION BACKEND` 添加或者删除 BE 实现计算组的扩缩容。









# 数据缓存

在存算分离的架构中，数据被存储在远程存储。Doris 数据库通过利用本地硬盘上的缓存来加速数据访问，并采用了一种先进的多队列 LRU（Least Recently Used）策略来高效管理缓存空间。这种策略特别优化了索引和元数据的访问路径，旨在最大化地缓存用户频繁访问的数据。针对多计算组（Compute Group）的应用场景，Doris 还提供了缓存预热功能，以便在新计算组建立时，能够迅速加载特定数据（如表或分区）到缓存中，从而提升查询性能。

## 多队列 LRU

### LRU

- LRU 通过维护一个数据访问队列来管理缓存。当数据被访问时，该数据会被移动到队列的前端。新加入缓存的数据同样会被置于队列前端，以防止其过早被淘汰。当缓存空间达到上限时，队列尾部的数据将优先被移除。

### TTL (Time-To-Live)

- TTL 策略确保新导入的数据在缓存中保留一段时间不被淘汰。在这段时间内，数据具有最高优先级，且所有 TTL 数据之间地位平等。当缓存空间不足时，系统会优先淘汰其它队列中的数据，以确保 TTL 数据能够被写入缓存。
- 应用场景：TTL 策略特别适用于希望在本地持久化的小规模数据表。对于常驻表，可以设置较长的 TTL 值来保护其数据；对于动态分区的数据表，可以根据 Hot Partition 的活跃时间设定相应的 TTL 值。
- 注意事项：目前系统不支持直接查看 TTL 数据在缓存中的占比。

### 多队列

- Doris 采用基于 LRU 的多队列策略，根据 TTL 属性和数据属性将数据分为四类，并分别置于 TTL 队列、Index 队列、NormalData 队列和 Disposable 队列中。设置了 TTL 属性的数据被放置到 TTL 队列，没有设置 TTL 属性的索引数据被放置到 Index 队列，没有设置 TTL 属性的索引数据被放置到 NormalData 队列，临时使用的数据被放置到 Disposable 队列中。
- 在数据读取和写入过程中，Doris 会地选择填充和读取的队列，以最大化缓存利用率。具体机制如下：

| 操作          | 未命中时填充的队列       | 写入数据时填充的队列     |
| ------------- | ------------------------ | ------------------------ |
| 导入          | TTL / Index / NormalData | TTL / Index / NormalData |
| 查询          | TTL / Index / NormalData | N/A                      |
| schema change | Disposable               | TTL / Index / NormalData |
| compaction    | Disposable               | TTL / Index / NormalData |
| 预热          | N/A                      | TTL / Index / NormalData |

### 淘汰

上述各类型缓存共同使用总缓存空间。根据重要程度的不同我们可以为它们划分比例。比例可以在 be 配置文件中通过 `file_cache_path` 设置，默认为：TTL : Normal : Index : Disposable = 50% : 30% : 10% : 10%。

这些比例不是硬性限制，Doris 会根据需要动态调整以充分利用空间来加速访问。例如用户如果不使用 TTL 类型的缓存，那么其它类型可以超过预设比例使用原本为 TTL 分配的空间。

缓存的淘汰有两种触发时机：垃圾清理或者缓存空间不足。当用户删除数据时，或是导入 compaction 任务结束时，会异步地对过期的缓存数据进行淘汰。写入缓存空间不足时，会按照 Disposable、Normal Data、Index、TTL 的顺序淘汰。例如：如果写入 Normal Data 时空间不足，那么 Doris 会依次淘汰 Disposable、Index、TTL 的部分数据（按照 LRU 的顺序）。注意我们不会将淘汰目标类型的数据全部淘汰再淘汰顺序中的下一个类型，而是至少会保留上述比例的空间以让其它类型也能正常工作。如果这个过程不能成功淘汰出足够的空间，那么将会触发自身类型的 LRU 淘汰。接着上面写 Normal Data 时空间不足例子，如果不能从其它类型中淘汰出足够的空间，此时 Normal Data 将从自身按照 LRU 顺序淘汰出数据。

其中特别注意的是，对于带有过期时间的 TTL 队列，其数据过期时会被移动到 Normal Data 队列，作为 Normal Data 参与淘汰。

## 缓存预热

在存算分离模式下，Doris 支持多计算组部署，各计算组间共享数据但不共享缓存。新计算组创建时，其缓存为空，可能影响查询性能。为此，Doris 提供缓存预热功能，允许用户从远端存储主动拉取数据至本地缓存。该功能支持以下三种模式：

- **计算组间预热**：将计算组 A 的缓存数据预热至计算组 B。Doris 定期收集各计算组在一段时间内被访问的表/分区的热点信息，并根据这些信息选择性地预热某些表/分区。
- **表数据预热**：指定将表 A 的数据预热至新计算组。
- **分区数据预热**：指定将表 A 的分区 `p1` 的数据预热至新计算组。

## Compute Group 扩缩容

Compute Group 扩缩容时，为了避免 Cache 波动，Doris 会首先对受影响的 Tablet 重新映射并预热数据。

## 缓存观测

### 热点信息

Doris 每 10 分钟收集各个计算组的缓存热点信息到内部系统表，您可以通过查询语句查看热点信息。 用户可以根据这些信息更好地规划缓存的使用。

备注

在 3.0.4 版本之前，可以使用 `SHOW CACHE HOTSPOT` 语句进行缓存热度信息统计查询。从 3.0.4 版本开始，不再支持使用 `SHOW CACHE HOTSPOT` 语句进行缓存热度信息统计查询。请直接访问系统表 `__internal_schema.cloud_cache_hotspot` 进行查询。

用户通常关注计算组和库表两个维度的缓存使用情况。以下提供了一些常用的查询语句以及示例。

#### 查看当前所有计算组中最频繁访问的表

```sql
-- 等价于 3.0.4 版本前的 SHOW CACHE HOTSPOT "/"
WITH t1 AS (
  SELECT
    cluster_id,
    cluster_name,
    table_id,
    table_name,
    insert_day,
    SUM(query_per_day) AS query_per_day_total,
    SUM(query_per_week) AS query_per_week_total
  FROM __internal_schema.cloud_cache_hotspot
  GROUP BY cluster_id, cluster_name, table_id, table_name, insert_day
)
SELECT
  cluster_id AS ComputeGroupId,
  cluster_name AS ComputeGroupName,
  table_id AS TableId,
  table_name AS TableName
FROM (
  SELECT
    ROW_NUMBER() OVER (
      PARTITION BY cluster_id
      ORDER BY insert_day DESC, query_per_day_total DESC, query_per_week_total DESC
    ) AS dr2,
    *
  FROM t1
) t2
WHERE dr2 = 1;
```



#### 查看某个计算组下的所有表中最频繁访问的表

查看计算组 `compute_group_name0` 下的所有表中最频繁访问的表

注意：将其中的 `cluster_name = "compute_group_name0"` 条件替换为实际的计算组名称。

```sql
-- 等价于 3.0.4 版本前的 SHOW CACHE HOTSPOT '/compute_group_name0';
WITH t1 AS (
  SELECT
    cluster_id,
    cluster_name,
    table_id,
    table_name,
    insert_day,
    SUM(query_per_day) AS query_per_day_total,
    SUM(query_per_week) AS query_per_week_total
  FROM __internal_schema.cloud_cache_hotspot
  WHERE cluster_name = "compute_group_name0" -- 替换为实际的计算组名称，例如 "default_compute_group"
  GROUP BY cluster_id, cluster_name, table_id, table_name, insert_day
)
SELECT
  cluster_id AS ComputeGroupId,
  cluster_name AS ComputeGroupName,
  table_id AS TableId,
  table_name AS TableName
FROM (
  SELECT
    ROW_NUMBER() OVER (
      PARTITION BY cluster_id
      ORDER BY insert_day DESC, query_per_day_total DESC, query_per_week_total DESC
    ) AS dr2,
    *
  FROM t1
) t2
WHERE dr2 = 1;
```



#### 查看具体计算组以及表缓存的访问最频繁的分区

查看计算组 `compute_group_name0` 下，表 `regression_test_cloud_load_copy_into_tpch_sf1_p1.customer` 的最频繁的分区。

注意：将其中的 `cluster_name = "compute_group_name0"` 和 `table_name = "regression_test_cloud_load_copy_into_tpch_sf1_p1.customer"` 条件替换为实际的计算组名称和库表名称。

```sql
-- 等价于 3.0.4 版本前的 SHOW CACHE HOTSPOT '/compute_group_name0/regression_test_cloud_load_copy_into_tpch_sf1_p1.customer';
SELECT
  partition_id AS PartitionId,
  partition_name AS PartitionName
FROM __internal_schema.cloud_cache_hotspot
WHERE
  cluster_name = "compute_group_name0" -- 替换为实际的计算组名称，例如 "default_compute_group"
  AND table_name = "regression_test_cloud_load_copy_into_tpch_sf1_p1.customer" -- 替换为实际的库表名称，例如 "db1.t1"
GROUP BY
  cluster_id,
  cluster_name,
  table_id,
  table_name,
  partition_id,
  partition_name;
```



### Cache 空间以及命中率

Doris BE 节点通过 `curl {be_ip}:{brpc_port}/vars` ( brpc_port 默认为 8060 ) 获取 cache 统计信息，指标项的名称开始为磁盘路径。

上述例子中指标前缀为 File Cache 的路径，例如前缀 "*mnt_disk1_gavinchou_debug_doris_cloud_be0_storage_file_cache*" 表示 "/mnt/disk1/gavinchou/debug/doris-cloud/be0_storage_file_cache/" 去掉前缀的部分为统计指标，比如 "file_cache_cache_size" 表示当前 路径的 File Cache 大小为 26111 字节

下表为全部的指标意义 (以下表示 Size 大小单位均为字节)

| 指标名称 (不包含路径前缀)                    | 语义                                                         |
| -------------------------------------------- | ------------------------------------------------------------ |
| file_cache_cache_size                        | 当前 File Cache 的总大小                                     |
| file_cache_disposable_queue_cache_size       | 当前 disposable 队列的大小                                   |
| file_cache_disposable_queue_element_count    | 当前 disposable 队列里的元素个数                             |
| file_cache_disposable_queue_evict_size       | 从启动到当前 disposable 队列总共淘汰的数据量大小             |
| file_cache_index_queue_cache_size            | 当前 index 队列的大小                                        |
| file_cache_index_queue_element_count         | 当前 index 队列里的元素个数                                  |
| file_cache_index_queue_evict_size            | 从启动到当前 index 队列总共淘汰的数据量大小                  |
| file_cache_normal_queue_cache_size           | 当前 normal 队列的大小                                       |
| file_cache_normal_queue_element_count        | 当前 normal 队列里的元素个数                                 |
| file_cache_normal_queue_evict_size           | 从启动到当前 normal 队列总共淘汰的数据量大小                 |
| file_cache_total_evict_size                  | 从启动到当前，整个 File Cache 总共淘汰的数据量大小           |
| file_cache_ttl_cache_evict_size              | 从启动到当前 TTL 队列总共淘汰的数据量大小                    |
| file_cache_ttl_cache_lru_queue_element_count | 当前 TTL 队列里的元素个数                                    |
| file_cache_ttl_cache_size                    | 当前 TTL 队列的大小                                          |
| file_cache_evict_by_heat_[A]_to_[B]          | 为了写入 B 缓存类型的数据而淘汰的 A 缓存类型的数据量（基于过期时间的淘汰方式） |
| file_cache_evict_by_size_[A]_to_[B]          | 为了写入 B 缓存类型的数据而淘汰的 A 缓存类型的数据量（基于空间的淘汰方式） |
| file_cache_evict_by_self_lru_[A]             | A 缓存类型的数据为了写入新数据而淘汰自身的数据量（基于 LRU 的淘汰方式） |

### SQL profile

SQL profile 中 cache 相关的指标在 SegmentIterator 下，包括

| 指标名称               | 语义                                       |
| ---------------------- | ------------------------------------------ |
| BytesScannedFromCache  | 从 File Cache 读取的数据量                 |
| BytesScannedFromRemote | 从远程存储读取的数据量                     |
| BytesWriteIntoCache    | 写入 File Cache 的数据量                   |
| LocalIOUseTimer        | 读取 File Cache 的耗时                     |
| NumLocalIOTotal        | 读取 File Cache 的次数                     |
| NumRemoteIOTotal       | 读取远程存储的次数                         |
| NumSkipCacheIOTotal    | 从远程存储读取并没有进入 File Cache 的次数 |
| RemoteIOUseTimer       | 读取远程存储的耗时                         |
| WriteCacheIOUseTimer   | 写 File Cache 的耗时                       |

您可以通过 [查询性能分析](https://doris.apache.org/zh-CN/docs/3.0/query-acceleration/performance-tuning-overview/analysis-tools#doris-profile) 查看查询性能分析。

## 使用方法

### 设置 TTL 策略

在建表时，设置相应的 PROPERTY，即可将该表的数据使用 TTL 策略进行缓存。

- `file_cache_ttl_seconds` : 新导入的数据期望在缓存中保留的时间，单位为秒。

```shell
CREATE TABLE IF NOT EXISTS customer (
  C_CUSTKEY     INTEGER NOT NULL,
  C_NAME        VARCHAR(25) NOT NULL,
  C_ADDRESS     VARCHAR(40) NOT NULL,
  C_NATIONKEY   INTEGER NOT NULL,
  C_PHONE       CHAR(15) NOT NULL,
  C_ACCTBAL     DECIMAL(15,2)   NOT NULL,
  C_MKTSEGMENT  CHAR(10) NOT NULL,
  C_COMMENT     VARCHAR(117) NOT NULL
)
DUPLICATE KEY(C_CUSTKEY, C_NAME)
DISTRIBUTED BY HASH(C_CUSTKEY) BUCKETS 32
PROPERTIES(
    "file_cache_ttl_seconds"="300"
)
```



上表中，所有新导入的数据将在缓存中被保留 300 秒。系统当前支持修改表的 TTL 时间，用户可以根据实际需求将 TTL 的时间延长或减短。

```sql
ALTER TABLE customer set ("file_cache_ttl_seconds"="3000");
```



备注

修改后的 TTL 值并不会立即生效，而会存在一定的延迟。

如果在建表时没有设置 TTL，用户同样可以通过执行 ALTER 语句来修改表的 TTL 属性。

### 缓存预热

- 计算组间预热，将 `compute_group_name0` 的缓存数据预热到 `compute_group_name1` 。

当执行以下 SQL 时，`compute_group_name1` 计算组会获取 `compute_group_name0` 计算组的访问信息，来尽可能还原出与 `compute_group_name0` 计算组一致的缓存。

```sql
WARM UP COMPUTE GROUP compute_group_name1 WITH COMPUTE GROUP compute_group_name0
```



- 表数据预热，将表 `customer` 的数据预热到 `compute_group_name1`。执行以下 SQL，可以将该表在远端存储上的数据全部拉取到本地。

```sql
WARM UP COMPUTE GROUP compute_group_name1 WITH TABLE customer
```



- 分区数据预热，将表 `customer` 的分区 `p1` 的数据预热到 `compute_group_name1`。执行以下 SQL，可以将该分区在远端存储上的数据全部拉取到本地。

```sql
WARM UP COMPUTE GROUP compute_group_name1 with TABLE customer PARTITION p1
```



上述三条缓存预热 SQL 均会返回一个 JobID 结果。例如：

```sql
WARM UP COMPUTE GROUP cloud_warm_up WITH TABLE test_warm_up;
```



然后可以通过以下 SQL 查看缓存预热进度。

```sql
SHOW WARM UP JOB WHERE ID = 13418; 
```



可根据 `FinishBatch` 和 `AllBatch` 判断当前任务进度，每个 Batch 的数据大小约为 10GB。目前，一个计算组中，同一时间内只支持执行一个预热 Job。用户可以停止正在进行的预热 Job。

```sql
CANCEL WARM UP JOB WHERE id = 13418;
```



## 实践案例

某用户拥有一系列数据表，总数据量超过 3TB，而可用缓存容量仅为 1.2TB。其中，访问频率较高的表有两张：一张是大小为 200MB 的维度表 (`dimension_table`)，另一张是大小为 100GB 的事实表 (`fact_table`)，后者每日都有新数据导入，并需要执行 T+1 查询操作。此外，其他大表访问频率不高。

在 LRU 缓存策略下，大表数据如果被查询访问，可能会替换掉需要常驻缓存的小表数据，造成性能波动。为了解决这个问题，用户采取 TTL 缓存策略，将两张表的 TTL 时间分别设置为 1 年和 1 天。

```shell
ALTER TABLE dimension_table set ("file_cache_ttl_seconds"="31536000");

ALTER TABLE fact_table set ("file_cache_ttl_seconds"="86400");
```



对于维度表，由于其数据量较小且变动不大，用户设置 1 年的 TTL 时间，以确保其数据在一年内都能被快速访问；对于事实表，用户每天需要进行一次表备份，然后进行全量导入，因此将其 TTL 时间设置为 1 天。







